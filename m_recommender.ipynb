{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique movies: 1682\n",
      "Number of unique users: 943\n",
      "Epoch 1/5\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 11.2776 - val_loss: 1.4139\n",
      "Epoch 2/5\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 1.1754 - val_loss: 0.9937\n",
      "Epoch 3/5\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - loss: 0.9142 - val_loss: 0.9371\n",
      "Epoch 4/5\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.8466 - val_loss: 0.9131\n",
      "Epoch 5/5\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - loss: 0.7817 - val_loss: 0.8966\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 960us/step\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "Recommended movie IDs for user 42: [[ 27]\n",
      " [301]\n",
      " [407]\n",
      " [ 21]\n",
      " [ 63]\n",
      " [173]\n",
      " [312]\n",
      " [271]\n",
      " [ 49]\n",
      " [317]]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load the MovieLens dataset\n",
    "ratings = tfds.load('movielens/100k-ratings', split=\"train\")\n",
    "movies = tfds.load('movielens/100k-movies', split=\"train\")\n",
    "\n",
    "# Preprocess the dataset to ensure IDs are integers and zero-indexed\n",
    "ratings = ratings.map(lambda x: {\n",
    "    'movie_id': tf.cast(tf.strings.to_number(x['movie_id']), tf.int32) - 1,  # zero-index movie_id\n",
    "    'user_id': tf.cast(tf.strings.to_number(x['user_id']), tf.int32) - 1,    # zero-index user_id\n",
    "    'user_rating': x['user_rating']\n",
    "})\n",
    "\n",
    "# Find the actual number of unique users and movies using reduce\n",
    "unique_movie_ids = ratings.map(lambda x: x['movie_id']).apply(tf.data.experimental.unique())\n",
    "unique_user_ids = ratings.map(lambda x: x['user_id']).apply(tf.data.experimental.unique())\n",
    "\n",
    "# Convert dataset to numpy arrays to calculate the size\n",
    "def get_unique_count(dataset):\n",
    "    count = len(list(dataset.as_numpy_iterator()))\n",
    "    return count\n",
    "\n",
    "num_movies = get_unique_count(unique_movie_ids)\n",
    "num_users = get_unique_count(unique_user_ids)\n",
    "\n",
    "print(f\"Number of unique movies: {num_movies}\")\n",
    "print(f\"Number of unique users: {num_users}\")\n",
    "\n",
    "# Create embedding model\n",
    "class MovieLensModel(tf.keras.Model):\n",
    "    def __init__(self, num_users, num_movies, embedding_dim):\n",
    "        super().__init__()\n",
    "        # Embedding layers\n",
    "        self.user_embedding = tf.keras.layers.Embedding(num_users, embedding_dim)\n",
    "        self.movie_embedding = tf.keras.layers.Embedding(num_movies, embedding_dim)\n",
    "        # Compute dot product between user and movie embeddings\n",
    "        self.dot = tf.keras.layers.Dot(axes=1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_id, movie_id = inputs\n",
    "        user_vec = self.user_embedding(user_id)\n",
    "        movie_vec = self.movie_embedding(movie_id)\n",
    "        return self.dot([user_vec, movie_vec])\n",
    "\n",
    "# Use actual unique values for num_users and num_movies\n",
    "embedding_dim = 50  # Size of the embedding vectors\n",
    "\n",
    "model = MovieLensModel(num_users, num_movies, embedding_dim)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Prepare training data (user_ids, movie_ids, ratings)\n",
    "def prepare_data(ratings):\n",
    "    user_ids = []\n",
    "    movie_ids = []\n",
    "    ratings_ = []\n",
    "\n",
    "    for rating in ratings:\n",
    "        user_ids.append(int(rating['user_id'].numpy()))\n",
    "        movie_ids.append(int(rating['movie_id'].numpy()))\n",
    "        ratings_.append(float(rating['user_rating'].numpy()))\n",
    "\n",
    "    return np.array(user_ids), np.array(movie_ids), np.array(ratings_)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_ratings = ratings.take(80000)\n",
    "test_ratings = ratings.skip(80000)\n",
    "\n",
    "# Prepare the data\n",
    "train_user_ids, train_movie_ids, train_ratings = prepare_data(train_ratings)\n",
    "test_user_ids, test_movie_ids, test_ratings = prepare_data(test_ratings)\n",
    "\n",
    "# Train the model\n",
    "model.fit([train_user_ids, train_movie_ids], train_ratings, epochs=5,\n",
    "          validation_data=([test_user_ids, test_movie_ids], test_ratings))\n",
    "\n",
    "# Predict ratings for new user-movie pairs\n",
    "predicted_ratings = model.predict([test_user_ids, test_movie_ids])\n",
    "\n",
    "# Recommend top movies for a user\n",
    "def recommend_movies(user_id, num_recommendations=10):\n",
    "    # Predict ratings for all movies for a specific user\n",
    "    movie_ids = np.arange(num_movies)\n",
    "    predicted_ratings = model.predict([np.full(movie_ids.shape, user_id), movie_ids])\n",
    "\n",
    "    # Get top movie IDs\n",
    "    top_movie_ids = np.argsort(predicted_ratings, axis=0)[-num_recommendations:]\n",
    "    return top_movie_ids\n",
    "\n",
    "# Example: Recommend top 10 movies for user 42\n",
    "recommendations = recommend_movies(42, 10)\n",
    "print(f\"Recommended movie IDs for user 42: {recommendations}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\akars\\AppData\\Local\\Temp\\ipykernel_4068\\2551746701.py:25: unique (from tensorflow.python.data.experimental.ops.unique) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.unique(...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\akars\\AppData\\Local\\Temp\\ipykernel_4068\\2551746701.py:25: unique (from tensorflow.python.data.experimental.ops.unique) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.unique(...)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique movies: 1682\n",
      "Number of unique users: 943\n",
      "Epoch 1/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 875us/step - loss: 11.2264 - val_loss: 1.4318\n",
      "Epoch 2/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 877us/step - loss: 1.1702 - val_loss: 0.9919\n",
      "Epoch 3/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 856us/step - loss: 0.8945 - val_loss: 0.9267\n",
      "Epoch 4/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 848us/step - loss: 0.8189 - val_loss: 0.9016\n",
      "Epoch 5/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 844us/step - loss: 0.7667 - val_loss: 0.8859\n",
      "Epoch 6/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 838us/step - loss: 0.7138 - val_loss: 0.8830\n",
      "Epoch 7/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 840us/step - loss: 0.6521 - val_loss: 0.8797\n",
      "Epoch 8/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 861us/step - loss: 0.6075 - val_loss: 0.8843\n",
      "Epoch 9/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 849us/step - loss: 0.5554 - val_loss: 0.8936\n",
      "Epoch 10/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 896us/step - loss: 0.4938 - val_loss: 0.9059\n",
      "Epoch 11/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 907us/step - loss: 0.4378 - val_loss: 0.9289\n",
      "Epoch 12/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 906us/step - loss: 0.3841 - val_loss: 0.9491\n",
      "Epoch 13/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 885us/step - loss: 0.3329 - val_loss: 0.9827\n",
      "Epoch 14/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 897us/step - loss: 0.2907 - val_loss: 1.0059\n",
      "Epoch 15/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 895us/step - loss: 0.2503 - val_loss: 1.0385\n",
      "Epoch 16/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 887us/step - loss: 0.2209 - val_loss: 1.0714\n",
      "Epoch 17/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 889us/step - loss: 0.1969 - val_loss: 1.0973\n",
      "Epoch 18/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 868us/step - loss: 0.1762 - val_loss: 1.1242\n",
      "Epoch 19/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 860us/step - loss: 0.1610 - val_loss: 1.1569\n",
      "Epoch 20/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 874us/step - loss: 0.1496 - val_loss: 1.1827\n",
      "Epoch 21/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 875us/step - loss: 0.1382 - val_loss: 1.2127\n",
      "Epoch 22/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 872us/step - loss: 0.1294 - val_loss: 1.2352\n",
      "Epoch 23/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 857us/step - loss: 0.1218 - val_loss: 1.2645\n",
      "Epoch 24/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 874us/step - loss: 0.1139 - val_loss: 1.2814\n",
      "Epoch 25/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 877us/step - loss: 0.1076 - val_loss: 1.3104\n",
      "\u001b[1m625/625\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 645us/step\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Recommended movies for user 43: ['Mirror Has Two Faces, The (1996)', 'Arrival, The (1996)', 'Titanic (1997)', 'Henry V (1989)', 'Saint, The (1997)', \"Devil's Advocate, The (1997)\", 'Boys of St. Vincent, The (1993)', 'English Patient, The (1996)', 'Omen, The (1976)', 'Much Ado About Nothing (1993)']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load the MovieLens dataset\n",
    "ratings = tfds.load('movielens/100k-ratings', split=\"train\")\n",
    "movies = tfds.load('movielens/100k-movies', split=\"train\")\n",
    "\n",
    "# Preprocess the dataset to ensure IDs are integers and zero-indexed\n",
    "ratings = ratings.map(lambda x: {\n",
    "    'movie_id': tf.cast(tf.strings.to_number(x['movie_id']), tf.int32) - 1,  # zero-index movie_id\n",
    "    'user_id': tf.cast(tf.strings.to_number(x['user_id']), tf.int32) - 1,    # zero-index user_id\n",
    "    'user_rating': x['user_rating']\n",
    "})\n",
    "\n",
    "# Get movie titles and movie IDs into a dictionary\n",
    "movie_titles = {}\n",
    "for movie in movies:\n",
    "    movie_id = int(movie['movie_id'].numpy()) - 1  # zero-index movie_id\n",
    "    movie_title = movie['movie_title'].numpy().decode('utf-8')\n",
    "    movie_titles[movie_id] = movie_title\n",
    "\n",
    "# Find the actual number of unique users and movies using reduce\n",
    "unique_movie_ids = ratings.map(lambda x: x['movie_id']).apply(tf.data.experimental.unique())\n",
    "unique_user_ids = ratings.map(lambda x: x['user_id']).apply(tf.data.experimental.unique())\n",
    "\n",
    "# Convert dataset to numpy arrays to calculate the size\n",
    "def get_unique_count(dataset):\n",
    "    count = len(list(dataset.as_numpy_iterator()))\n",
    "    return count\n",
    "\n",
    "num_movies = get_unique_count(unique_movie_ids)\n",
    "num_users = get_unique_count(unique_user_ids)\n",
    "\n",
    "print(f\"Number of unique movies: {num_movies}\")\n",
    "print(f\"Number of unique users: {num_users}\")\n",
    "\n",
    "# Create embedding model\n",
    "class MovieLensModel(tf.keras.Model):\n",
    "    def __init__(self, num_users, num_movies, embedding_dim):\n",
    "        super().__init__()\n",
    "        # Embedding layers\n",
    "        self.user_embedding = tf.keras.layers.Embedding(num_users, embedding_dim)\n",
    "        self.movie_embedding = tf.keras.layers.Embedding(num_movies, embedding_dim)\n",
    "        # Compute dot product between user and movie embeddings\n",
    "        self.dot = tf.keras.layers.Dot(axes=1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_id, movie_id = inputs\n",
    "        user_vec = self.user_embedding(user_id)\n",
    "        movie_vec = self.movie_embedding(movie_id)\n",
    "        return self.dot([user_vec, movie_vec])\n",
    "\n",
    "# Use actual unique values for num_users and num_movies\n",
    "embedding_dim = 50  # Size of the embedding vectors\n",
    "\n",
    "model = MovieLensModel(num_users, num_movies, embedding_dim)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# Prepare training data (user_ids, movie_ids, ratings)\n",
    "def prepare_data(ratings):\n",
    "    user_ids = []\n",
    "    movie_ids = []\n",
    "    ratings_ = []\n",
    "\n",
    "    for rating in ratings:\n",
    "        user_ids.append(int(rating['user_id'].numpy()))\n",
    "        movie_ids.append(int(rating['movie_id'].numpy()))\n",
    "        ratings_.append(float(rating['user_rating'].numpy()))\n",
    "\n",
    "    return np.array(user_ids), np.array(movie_ids), np.array(ratings_)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_ratings = ratings.take(80000)\n",
    "test_ratings = ratings.skip(80000)\n",
    "\n",
    "# Prepare the data\n",
    "train_user_ids, train_movie_ids, train_ratings = prepare_data(train_ratings)\n",
    "test_user_ids, test_movie_ids, test_ratings = prepare_data(test_ratings)\n",
    "\n",
    "# Train the model\n",
    "model.fit([train_user_ids, train_movie_ids], train_ratings, epochs=25,\n",
    "          validation_data=([test_user_ids, test_movie_ids], test_ratings))\n",
    "\n",
    "# Predict ratings for new user-movie pairs\n",
    "predicted_ratings = model.predict([test_user_ids, test_movie_ids])\n",
    "\n",
    "# Recommend top movies for a user\n",
    "def recommend_movies(user_id, num_recommendations=10):\n",
    "    # Predict ratings for all movies for a specific user\n",
    "    movie_ids = np.arange(num_movies)\n",
    "    predicted_ratings = model.predict([np.full(movie_ids.shape, user_id), movie_ids])\n",
    "\n",
    "    # Get top movie IDs\n",
    "    top_movie_ids = np.argsort(predicted_ratings.flatten())[-num_recommendations:]\n",
    "    return top_movie_ids\n",
    "\n",
    "# Example: Recommend top 10 movies for user 42\n",
    "user_id = 42  # For zero-indexed user ID\n",
    "recommendations = recommend_movies(user_id, 10)\n",
    "\n",
    "# Convert movie IDs to titles\n",
    "recommended_movie_titles = [movie_titles[movie_id] for movie_id in recommendations]\n",
    "\n",
    "print(f\"Recommended movies for user {user_id + 1}: {recommended_movie_titles}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique movies: 1682\n",
      "Number of unique users: 943\n",
      "Epoch 1/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 3.5999 - val_loss: 3.5260\n",
      "Epoch 2/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 3.5286 - val_loss: 3.5260\n",
      "Epoch 3/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 3.5311 - val_loss: 3.5260\n",
      "Epoch 4/25\n",
      "\u001b[1m2500/2500\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - loss: 3.5343 - val_loss: 3.5260\n",
      "\u001b[1m53/53\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step\n",
      "Recommended movies for user 43: ['Low Down Dirty Shame, A (1994)', 'Starship Troopers (1997)', 'Believers, The (1987)', 'Striptease (1996)', 'Cabin Boy (1994)', 'Gold Diggers: The Secret of Bear Mountain (1995)', 'Crossing Guard, The (1995)', 'Crows and Sparrows (1949)', \"Pharaoh's Army (1995)\", 'Convent, The (Convento, O) (1995)']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Load the MovieLens dataset\n",
    "ratings = tfds.load('movielens/100k-ratings', split=\"train\")\n",
    "movies = tfds.load('movielens/100k-movies', split=\"train\")\n",
    "\n",
    "# Preprocess the dataset to ensure IDs are integers and zero-indexed\n",
    "ratings = ratings.map(lambda x: {\n",
    "    'movie_id': tf.cast(tf.strings.to_number(x['movie_id']), tf.int32) - 1,\n",
    "    'user_id': tf.cast(tf.strings.to_number(x['user_id']), tf.int32) - 1,\n",
    "    'user_rating': x['user_rating']\n",
    "})\n",
    "\n",
    "# Get movie titles into a dictionary\n",
    "movie_titles = {}\n",
    "for movie in movies:\n",
    "    movie_id = int(movie['movie_id'].numpy()) - 1\n",
    "    movie_title = movie['movie_title'].numpy().decode('utf-8')\n",
    "    movie_titles[movie_id] = movie_title\n",
    "\n",
    "# Find the actual number of unique users and movies\n",
    "num_movies = ratings.map(lambda x: x['movie_id']).apply(tf.data.experimental.unique()).reduce(0, lambda x, _: x + 1)\n",
    "num_users = ratings.map(lambda x: x['user_id']).apply(tf.data.experimental.unique()).reduce(0, lambda x, _: x + 1)\n",
    "\n",
    "print(f\"Number of unique movies: {num_movies.numpy()}\")\n",
    "print(f\"Number of unique users: {num_users.numpy()}\")\n",
    "\n",
    "# Create embedding model\n",
    "class MovieLensModel(tf.keras.Model):\n",
    "    def __init__(self, num_users, num_movies, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.user_embedding = tf.keras.layers.Embedding(num_users, embedding_dim, embeddings_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.movie_embedding = tf.keras.layers.Embedding(num_movies, embedding_dim, embeddings_regularizer=tf.keras.regularizers.l2(0.01))\n",
    "        self.dot = tf.keras.layers.Dot(axes=1)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        user_id, movie_id = inputs\n",
    "        user_vec = self.user_embedding(user_id)\n",
    "        movie_vec = self.movie_embedding(movie_id)\n",
    "        return self.dot([user_vec, movie_vec])\n",
    "\n",
    "# Use a larger embedding dimension\n",
    "embedding_dim = 100\n",
    "\n",
    "model = MovieLensModel(num_users.numpy(), num_movies.numpy(), embedding_dim)\n",
    "\n",
    "# Compile the model with a different loss function\n",
    "model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "# Prepare training data\n",
    "def prepare_data(ratings):\n",
    "    user_ids, movie_ids, ratings_ = [], [], []\n",
    "    for rating in ratings:\n",
    "        user_ids.append(int(rating['user_id'].numpy()))\n",
    "        movie_ids.append(int(rating['movie_id'].numpy()))\n",
    "        ratings_.append(float(rating['user_rating'].numpy()))\n",
    "    return np.array(user_ids), np.array(movie_ids), np.array(ratings_)\n",
    "\n",
    "# Split data into training and validation sets\n",
    "train_ratings = ratings.take(80000)\n",
    "test_ratings = ratings.skip(80000)\n",
    "\n",
    "train_user_ids, train_movie_ids, train_ratings = prepare_data(train_ratings)\n",
    "test_user_ids, test_movie_ids, test_ratings = prepare_data(test_ratings)\n",
    "\n",
    "# Implement EarlyStopping\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model\n",
    "model.fit([train_user_ids, train_movie_ids], train_ratings, epochs=25,\n",
    "          validation_data=([test_user_ids, test_movie_ids], test_ratings),\n",
    "          callbacks=[early_stopping])\n",
    "\n",
    "# Recommend top movies for a user\n",
    "def recommend_movies(user_id, num_recommendations=10):\n",
    "    movie_ids = np.arange(num_movies.numpy())\n",
    "    predicted_ratings = model.predict([np.full(movie_ids.shape, user_id), movie_ids])\n",
    "    top_movie_ids = np.argsort(predicted_ratings.flatten())[-num_recommendations:]\n",
    "    return top_movie_ids\n",
    "\n",
    "# Example: Recommend top 10 movies for user 42\n",
    "user_id = 42\n",
    "recommendations = recommend_movies(user_id, 10)\n",
    "\n",
    "# Convert movie IDs to titles\n",
    "recommended_movie_titles = [movie_titles[movie_id] for movie_id in recommendations]\n",
    "\n",
    "print(f\"Recommended movies for user {user_id + 1}: {recommended_movie_titles}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
